{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for easy reference and modification\n",
    "GROUPBY_COL = 'unique_id'\n",
    "\n",
    "DATETIME_COL = 'datetime'\n",
    "\n",
    "TARGET_COL = 'vehicle_type'\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    'vehicle_speed',\n",
    "    'vehicle_angle_sine', 'vehicle_angle_cosine',\n",
    "    'vehicle_x', 'vehicle_y', 'vehicle_z'\n",
    "]\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 10\n",
    "NUM_FEATURES = 6\n",
    "NUM_LAYERS = 2\n",
    "HIDDEN_SIZE = 128\n",
    "DROPOUT = 0.5\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "privateuseone:0\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "\n",
    "# Third-party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# PyTorch and related imports\n",
    "import torch\n",
    "import torch_directml\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence, pack_padded_sequence\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "dml = torch_directml.device()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(dml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions and Class Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For creating dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(df):\n",
    "    sequences = []\n",
    "    for vehicle_id, group in df.groupby(GROUPBY_COL):\n",
    "        sorted_group = group.sort_values(by=[DATETIME_COL]).copy()\n",
    "        sequence_features = sorted_group[FEATURE_COLS].values\n",
    "        label = sorted_group.iloc[0][TARGET_COL]\n",
    "        sequences.append((sequence_features, label, vehicle_id))\n",
    "    return sequences\n",
    "\n",
    "class VehicleDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.sequences = create_sequences(df)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence, label, vehicle_id = self.sequences[idx]\n",
    "        return torch.tensor(sequence, dtype=torch.float32), torch.tensor(label, dtype=torch.long), vehicle_id\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences, labels, vehicle_ids = zip(*batch)\n",
    "\n",
    "    # Clone and detach the tensors in sequences before padding\n",
    "    padded_sequences = pad_sequence([seq.clone().detach() for seq in sequences], batch_first=True)\n",
    "\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences])\n",
    "\n",
    "    return padded_sequences, labels_tensor, vehicle_ids, lengths\n",
    "\n",
    "# ######################## USING GENERATORS ########################\n",
    "# def create_sequences_generator(df):\n",
    "#     for vehicle_id, group in df.groupby(GROUPBY_COL):\n",
    "#          # Ensure that data is sorted by  datetime\n",
    "#         group[DATETIME_COL] = pd.to_datetime(group[DATETIME_COL])\n",
    "#         group.sort_values(by=[DATETIME_COL], inplace=True)\n",
    "\n",
    "#         sequence_features = group[FEATURE_COLS].values\n",
    "#         label = group.iloc[0][TARGET_COL]\n",
    "#         yield sequence_features, label, vehicle_id\n",
    "\n",
    "# class VehicleIterableDataset(IterableDataset):\n",
    "#     def __init__(self, df):\n",
    "#         # IterableDataset stores the raw data and parameters needed to create the generator\n",
    "#         self.df = df\n",
    "#         self.groupby_col = GROUPBY_COL\n",
    "#         self.feature_cols = FEATURE_COLS\n",
    "#         self.target_col = TARGET_COL\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         # Re-initialises the generator each time it is called, as generators are exhausted after one complete iteration\n",
    "#         return create_sequences_generator(self.df)\n",
    "\n",
    "# def collate_fn_generator(batch):\n",
    "#     # Separate features, labels, and vehicle IDs\n",
    "#     sequences, labels, vehicle_ids = zip(*batch)\n",
    "\n",
    "#     # Pad sequences to have the same length\n",
    "#     padded_sequences = pad_sequence([torch.tensor(seq, dtype=torch.float32) for seq in sequences], batch_first=True)\n",
    "\n",
    "#     # Convert labels to tensor\n",
    "#     labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "#     # Calculate lengths of sequences\n",
    "#     lengths = torch.tensor([len(seq) for seq in sequences])\n",
    "\n",
    "#     return padded_sequences, labels_tensor, vehicle_ids, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    The output of an LSTM layer in PyTorch is a 3D tensor, and each dim represents:\n",
    "    1. Batch size: The batch size of the DataLoader.\n",
    "        In a batch, multiple sequences are processed simultaneously by the LSTM.\n",
    "        Each item in this dimension corresponds to a different sequence in the batch.\n",
    "    2. Sequence length: The number of rows in each sequence.\n",
    "        When padding is used to handle variable-length sequences,\n",
    "        this dimension represents the max length in the batch.\n",
    "    3. Hidden size: A hyperparameter when creating the LSTM layer.\n",
    "        The size of the LSTM's hidden state and\n",
    "        also the output features from the LSTM cell at each time step.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, dropout):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # Pack the padded sequences\n",
    "        x_packed = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        packed_output, (hidden, cell) = self.lstm(x_packed)\n",
    "\n",
    "        # Unpack the output\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        # Extract the hidden state of the last time step\n",
    "        last_hidden = hidden[-1]\n",
    "\n",
    "        # Pass through the linear layer\n",
    "        out = self.fc(last_hidden)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Utility class for early stopping\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f\"Validation loss decreased ({self.val_loss_min:.6f} to {val_loss:.6f}). Saving model...\")\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"\n",
    "    Class to encapsulate all methods and utilities for training and evaluating a model.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, criterion, optimizer, scheduler=None, device=None):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.criterion.to(self.device)\n",
    "\n",
    "\n",
    "    def train_for_one_epoch(self, data_loader, print_every=10):\n",
    "        \"\"\"\n",
    "        Train the model for one epoch, iterating over batches from the data_loader.\n",
    "\n",
    "        Args:\n",
    "            data_loader (DataLoader): The PyTorch DataLoader to fetch data from.\n",
    "            print_every (int): Interval of batches after which to print training metrics.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the average loss, accuracy, and F1 score for the epoch.\n",
    "        \"\"\"\n",
    "\n",
    "        # Set the model to training mode\n",
    "        self.model.train()\n",
    "\n",
    "        # Initialize accumulators for loss and accuracy metrics\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        all_labels, all_predictions = [], []\n",
    "\n",
    "        # Process each batch from the data loader\n",
    "        for batch_count, (sequences, labels, _, lengths) in enumerate(data_loader):\n",
    "            # Move data to the appropriate device (GPU or CPU)\n",
    "            sequences, labels = sequences.to(self.device), labels.to(self.device)\n",
    "\n",
    "            # Zero the gradients before running the forward pass.\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass: Compute predicted y by passing data through the model\n",
    "            outputs = self.model(sequences, lengths)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = self.criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Perform a single optimization step (parameter update)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Update training loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate batch accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Extend the list for later calculating whole epoch metrics\n",
    "            all_labels.extend(labels.tolist())\n",
    "            all_predictions.extend(predicted.tolist())\n",
    "\n",
    "            # Print training metrics every \"print_every\" batches\n",
    "            if (batch_count + 1) % print_every == 0:\n",
    "                avg_loss = total_loss / (batch_count + 1)\n",
    "                accuracy = correct / total\n",
    "                batch_f1 = f1_score(labels.cpu(), predicted.cpu(), average='weighted')\n",
    "                print(f\"Batch [{batch_count + 1}/{len(data_loader)}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}, F1: {batch_f1:.4f}\")\n",
    "\n",
    "        # Scheduler step if scheduler is set\n",
    "        if self.scheduler:\n",
    "            self.scheduler.step()\n",
    "\n",
    "        # Calculate average metrics over all batches\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        accuracy = correct / total\n",
    "        f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "        return avg_loss, accuracy, f1\n",
    "\n",
    "\n",
    "    def evaluate(self, data_loader):\n",
    "        \"\"\"\n",
    "        Evaluate the model on a given dataset using the specified data loader.\n",
    "\n",
    "        Args:\n",
    "            data_loader (DataLoader): The DataLoader to provide batches of data.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the average loss, accuracy, and F1 score for the evaluation.\n",
    "        \"\"\"\n",
    "\n",
    "        # Set the model to evaluation mode. This will turn off dropout and batch normalization\n",
    "        self.model.eval()\n",
    "\n",
    "        # Initialize accumulators for calculating average loss, accuracy, and F1 score\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        all_labels, all_predictions = [], []\n",
    "\n",
    "        # Disable gradient calculations for efficiency since gradients are not needed for evaluation\n",
    "        with torch.no_grad():\n",
    "            for sequences, labels, _, lengths in data_loader:\n",
    "                # Ensure data is on the correct device (GPU or CPU) to avoid unnecessary transfers\n",
    "                sequences, labels = sequences.to(self.device), labels.to(self.device)\n",
    "\n",
    "                # Forward pass: compute the model's predictions for the batch\n",
    "                outputs = self.model(sequences, lengths)\n",
    "\n",
    "                # Compute loss for the batch using the criterion defined for the model\n",
    "                loss = self.criterion(outputs, labels)\n",
    "\n",
    "                # Accumulate the loss for averaging later\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Compute prediction accuracy for the batch\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)  # Count total labels processed in this batch\n",
    "                correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "\n",
    "                # Store all labels and predictions for calculating global F1 score later\n",
    "                all_labels.extend(labels.tolist())\n",
    "                all_predictions.extend(predicted.tolist())\n",
    "\n",
    "        # Calculate and return average loss, accuracy, and F1 score across all batches\n",
    "        avg_loss = total_loss / len(data_loader)  # Average loss across batches\n",
    "        accuracy = correct / total  # Overall accuracy\n",
    "        f1 = f1_score(all_labels, all_predictions, average='weighted')  # Overall F1 score\n",
    "\n",
    "        return avg_loss, accuracy, f1\n",
    "\n",
    "    \n",
    "    def train_loop(self, train_loader, val_loader, num_epochs=100, print_every=10, save_internal=False, save_interval=10, filename=\"model\", save_dir=\"../\"):\n",
    "        \"\"\"\n",
    "        Full training loop including training and validation evaluation for a given number of epochs.\n",
    "        \"\"\"\n",
    "        # Ensure save directories exist\n",
    "        model_save_dir = os.path.join(save_dir, \"models\")\n",
    "        img_save_dir = os.path.join(save_dir, \"imgs\")\n",
    "        os.makedirs(model_save_dir, exist_ok=True)\n",
    "        os.makedirs(img_save_dir, exist_ok=True)\n",
    "\n",
    "        # Initialize early stopping utility\n",
    "        early_stopping = EarlyStopping(patience=7, verbose=True)\n",
    "\n",
    "        # Initialize lists to track metrics\n",
    "        train_losses, train_accs, train_f1s = [], [], []\n",
    "        val_losses, val_accs, val_f1s = [], [], []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "            # Train for one epoch and collect metrics\n",
    "            train_loss, train_acc, train_f1 = self.train_for_one_epoch(train_loader, print_every)\n",
    "            train_losses.append(train_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            train_f1s.append(train_f1)\n",
    "\n",
    "            # Evaluate on the validation set\n",
    "            val_loss, val_acc, val_f1 = self.evaluate(val_loader)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accs.append(val_acc)\n",
    "            val_f1s.append(val_f1)\n",
    "            print(f\"Validation - Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}, F1: {val_f1:.4f}\\n\")\n",
    "\n",
    "            # Early stopping check\n",
    "            early_stopping(val_loss, self.model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "            # Save model periodically\n",
    "            if save_internal:\n",
    "                if (epoch + 1) % save_interval == 0:\n",
    "                    internal_filename = f\"{filename}_epoch{epoch + 1}\"\n",
    "                    self.save_checkpoint(epoch, model_save_dir, internal_filename)\n",
    "\n",
    "        # Save the final model\n",
    "        final_model_path = os.path.join(model_save_dir, f\"{filename}_final.pth\")\n",
    "        torch.save(self.model.state_dict(), final_model_path)\n",
    "        print(f\"Final model saved to {final_model_path}\")\n",
    "\n",
    "        # Plot and save metrics\n",
    "        self.plot_metrics(train_losses, val_losses, train_accs, val_accs, train_f1s, val_f1s, num_epochs, img_save_dir, filename)\n",
    "\n",
    "    def save_checkpoint(self, epoch, save_dir, filename):\n",
    "        \"\"\"\n",
    "        Save model checkpoint.\n",
    "        \"\"\"\n",
    "        save_path = os.path.join(save_dir, f\"{filename}_epoch_{epoch + 1}.pth\")\n",
    "        torch.save(self.model.state_dict(), save_path)\n",
    "        print(f\"Model saved to {save_path}\")\n",
    "\n",
    "    def plot_metrics(self, train_losses, val_losses, train_accs, val_accs, train_f1s, val_f1s, num_epochs, img_save_dir, filename):\n",
    "        \"\"\"\n",
    "        Plot and save training and validation metrics.\n",
    "        \"\"\"\n",
    "        epochs = list(range(1, num_epochs + 1))\n",
    "        plt.figure(figsize=(18, 6))\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(epochs, train_losses, 'r-', label='Train Loss')\n",
    "        plt.plot(epochs, val_losses, 'b-', label='Validation Loss')\n",
    "        plt.title('Loss Over Epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(epochs, train_accs, 'r-', label='Train Accuracy')\n",
    "        plt.plot(epochs, val_accs, 'b-', label='Validation Accuracy')\n",
    "        plt.title('Accuracy Over Epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(epochs, train_f1s, 'r-', label='Train F1 Score')\n",
    "        plt.plot(epochs, val_f1s, 'b-', label='Validation F1 Score')\n",
    "        plt.title('F1 Score Over Epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plot_filepath = os.path.join(img_save_dir, f\"metrics_{filename}.png\")\n",
    "        plt.savefig(plot_filepath)\n",
    "        print(f\"Metrics plot saved to {plot_filepath}\")\n",
    "\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape Data for input to LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/train_data.csv')\n",
    "train_df[DATETIME_COL] = pd.to_datetime(train_df[DATETIME_COL])\n",
    "\n",
    "val_df = pd.read_csv('../data/val_data.csv')\n",
    "val_df[DATETIME_COL] = pd.to_datetime(val_df[DATETIME_COL])\n",
    "\n",
    "test_df = pd.read_csv('../data/test_data.csv')\n",
    "test_df[DATETIME_COL] = pd.to_datetime(test_df[DATETIME_COL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating datasets and dataloaders\n",
    "train_dataset = VehicleDataset(train_df)\n",
    "val_dataset = VehicleDataset(val_df)\n",
    "test_dataset = VehicleDataset(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "# test_loader1 = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# assert(len(test_loader) == len(test_loader1))\n",
    "# for i, batch in enumerate(test_loader):\n",
    "#     for j, batch1 in enumerate(test_loader1):\n",
    "#         if (i == j):\n",
    "#             assert(torch.all(torch.eq(batch[0], batch1[0])))\n",
    "#             assert(torch.all(torch.eq(batch[1], batch1[1])))\n",
    "#             assert(batch[2] == batch1[2])\n",
    "#             assert(torch.all(torch.eq(batch[3], batch1[3])))\n",
    "#         else:\n",
    "#             continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise model with hyperparameters\n",
    "model = LSTMClassifier(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    input_size=NUM_FEATURES,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    dropout=DROPOUT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise optimiser and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Adam optimizer with CyclicLR scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "lr_scheduler = CyclicLR(optimizer, base_lr=0.0001, max_lr=0.01,\n",
    "                         step_size_up=len(train_df), cycle_momentum=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise criterion as Cross Entropy Loss for multiclassification task\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create weighted criterion\n",
    "class_frequencies = train_df.groupby(TARGET_COL)[GROUPBY_COL].nunique()\n",
    "class_weights = 1.0 / class_frequencies\n",
    "class_weights = class_weights / class_weights.min() # Normalize the weights so that the smallest one is 1.0 (optional)\n",
    "class_weights_tensor = torch.tensor(class_weights.values, dtype=torch.float32)\n",
    "weighted_criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate ModelTrainer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': LSTMClassifier(\n",
       "   (lstm): LSTM(6, 128, num_layers=2, batch_first=True, dropout=0.5)\n",
       "   (fc): Linear(in_features=128, out_features=10, bias=True)\n",
       " ),\n",
       " 'criterion': CrossEntropyLoss(),\n",
       " 'optimizer': Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     differentiable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     fused: None\n",
       "     initial_lr: 0.0001\n",
       "     lr: 0.0001\n",
       "     maximize: False\n",
       "     weight_decay: 0\n",
       " ),\n",
       " 'scheduler': <torch.optim.lr_scheduler.CyclicLR at 0x7fac6c6ba170>,\n",
       " 'device': device(type='privateuseone', index=0)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = ModelTrainer(\n",
    "    model, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    scheduler=lr_scheduler, \n",
    "    device=dml\n",
    ")\n",
    "trainer.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model for num epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_loop(\n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    num_epochs=NUM_EPOCHS, \n",
    "    print_every=10, \n",
    "    save_internal=True,\n",
    "    save_interval=2, \n",
    "    filename=\"LSTM_test\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "duolux_v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for easy reference and modification\n",
    "GROUPBY_COL = 'unique_id'\n",
    "\n",
    "DATETIME_COL = 'datetime'\n",
    "\n",
    "TARGET_COL = 'vehicle_type'\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    'vehicle_speed',\n",
    "    'vehicle_angle_sine', 'vehicle_angle_cosine',\n",
    "    'vehicle_x', 'vehicle_y', 'vehicle_z'\n",
    "]\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "\n",
    "# PyTorch and related imports\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, IterableDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions and Class Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split_custom(df, test_size=0.2, val_size=0.1, min_per_class=1):\n",
    "    \"\"\"\n",
    "    Splits data into training, validation, and test sets.\n",
    "    Ensures that a vehicle_id only appears in one of the datasets.\n",
    "    Ensures that all possible classes are represented in each set.\n",
    "    In the event where a class has not enough vehicle_ids for each set to have at least one vehicle_id,\n",
    "    prioritise the train set to represent all possible classes.\n",
    "    \"\"\"\n",
    "    np.random.seed(0)  # Seed for reproducibility\n",
    "\n",
    "    # Get unique IDs and classes\n",
    "    unique_ids = df[GROUPBY_COL].unique()\n",
    "    classes = df[TARGET_COL].unique()\n",
    "\n",
    "    # Initialize lists to store IDs for each dataset\n",
    "    train_ids, val_ids, test_ids = [], [], []\n",
    "\n",
    "    for cls in classes:\n",
    "        ids_for_class = df[df[TARGET_COL] == cls][GROUPBY_COL].unique()\n",
    "        np.random.shuffle(ids_for_class)\n",
    "\n",
    "        # Allocate IDs to train, val, and test sets\n",
    "        class_count = len(ids_for_class)\n",
    "        train_end = min(min_per_class, class_count)\n",
    "        val_end = min(train_end + min_per_class, class_count)\n",
    "        test_end = min(val_end + min_per_class, class_count)\n",
    "\n",
    "        train_ids.extend(ids_for_class[:train_end])\n",
    "        val_ids.extend(ids_for_class[train_end:val_end])\n",
    "        test_ids.extend(ids_for_class[val_end:test_end])\n",
    "\n",
    "    # Shuffle remaining IDs after removing selected train IDs\n",
    "    reserved_ids = set(train_ids + val_ids + test_ids)\n",
    "    remaining_ids = list(set(unique_ids) - reserved_ids)\n",
    "    np.random.shuffle(remaining_ids)\n",
    "\n",
    "    # Calculate split sizes for remaining IDs\n",
    "    total_remaining = len(remaining_ids)\n",
    "    test_count = int(total_remaining * test_size)\n",
    "    val_count = int(total_remaining * val_size)\n",
    "\n",
    "    # Assign remaining vehicle IDs to test and validation sets\n",
    "    test_ids += remaining_ids[:test_count]\n",
    "    val_ids += remaining_ids[test_count:test_count + val_count]\n",
    "    train_ids += remaining_ids[test_count + val_count:]\n",
    "\n",
    "    # Create split DataFrames\n",
    "    train_df = df[df[GROUPBY_COL].isin(train_ids)]\n",
    "    val_df = df[df[GROUPBY_COL].isin(val_ids)]\n",
    "    test_df = df[df[GROUPBY_COL].isin(test_ids)]\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def create_sequences(df):\n",
    "    sequences = []\n",
    "    for vehicle_id, group in df.groupby(GROUPBY_COL):\n",
    "        sorted_group = group.sort_values(by=[DATETIME_COL]).copy()\n",
    "        sequence_features = sorted_group[FEATURE_COLS].values\n",
    "        label = sorted_group.iloc[0][TARGET_COL]\n",
    "        sequences.append((sequence_features, label, vehicle_id))\n",
    "    return sequences\n",
    "\n",
    "class VehicleDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.sequences = create_sequences(df)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence, label, vehicle_id = self.sequences[idx]\n",
    "        return torch.tensor(sequence, dtype=torch.float32), torch.tensor(label, dtype=torch.long), vehicle_id\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences, labels, vehicle_ids = zip(*batch)\n",
    "\n",
    "    # Clone and detach the tensors in sequences before padding\n",
    "    padded_sequences = pad_sequence([seq.clone().detach() for seq in sequences], batch_first=True)\n",
    "\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences])\n",
    "\n",
    "    return padded_sequences, labels_tensor, vehicle_ids, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## USING GENERATORS ########################\n",
    "def create_sequences_generator(df):\n",
    "    for vehicle_id, group in df.groupby(GROUPBY_COL):\n",
    "         # Ensure that data is sorted by  datetime\n",
    "        group[DATETIME_COL] = pd.to_datetime(group[DATETIME_COL])\n",
    "        group.sort_values(by=[DATETIME_COL], inplace=True)\n",
    "\n",
    "        sequence_features = group[FEATURE_COLS].values\n",
    "        label = group.iloc[0][TARGET_COL]\n",
    "        yield sequence_features, label, vehicle_id\n",
    "\n",
    "class VehicleIterableDataset(IterableDataset):\n",
    "    def __init__(self, df):\n",
    "        # IterableDataset stores the raw data and parameters needed to create the generator\n",
    "        self.df = df\n",
    "        self.groupby_col = GROUPBY_COL\n",
    "        self.feature_cols = FEATURE_COLS\n",
    "        self.target_col = TARGET_COL\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Re-initialises the generator each time it is called, as generators are exhausted after one complete iteration\n",
    "        return create_sequences_generator(self.df)\n",
    "\n",
    "def collate_fn_generator(batch):\n",
    "    # Separate features, labels, and vehicle IDs\n",
    "    sequences, labels, vehicle_ids = zip(*batch)\n",
    "\n",
    "    # Pad sequences to have the same length\n",
    "    padded_sequences = pad_sequence([torch.tensor(seq, dtype=torch.float32) for seq in sequences], batch_first=True)\n",
    "\n",
    "    # Convert labels to tensor\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    # Calculate lengths of sequences\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences])\n",
    "\n",
    "    return padded_sequences, labels_tensor, vehicle_ids, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load resampled data into df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1228962 entries, 0 to 1228961\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count    Dtype         \n",
      "---  ------                --------------    -----         \n",
      " 0   unique_id             1228962 non-null  object        \n",
      " 1   datetime              1228962 non-null  datetime64[ns]\n",
      " 2   vehicle_id            1228962 non-null  object        \n",
      " 3   timestep_time         1228962 non-null  float64       \n",
      " 4   vehicle_type          1228962 non-null  object        \n",
      " 5   vehicle_speed         1228962 non-null  float64       \n",
      " 6   vehicle_x             1228962 non-null  float64       \n",
      " 7   vehicle_y             1228962 non-null  float64       \n",
      " 8   vehicle_z             1228962 non-null  float64       \n",
      " 9   vehicle_angle         1228962 non-null  float64       \n",
      " 10  vehicle_angle_sine    1228962 non-null  float64       \n",
      " 11  vehicle_angle_cosine  1228962 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(8), object(3)\n",
      "memory usage: 112.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('resampled_vehicle_data.csv')\n",
    "df[DATETIME_COL] = pd.to_datetime(df[DATETIME_COL])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Re-labelling classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following [this notebook on GitHub](https://github.com/pgrandinetti/standard-traffic-data/blob/main/knowledge/Urban_Traffic_Data_Statistical_Analysis.ipynb), we:\n",
    "\n",
    "- Fill in null values with `pedestrian`\n",
    "- Rename all vehicle types types that start with `passenger` as `car` as these instances are a subclass of car\n",
    "- Rename vehicle types suffixed with bicycle (`fastbicycle`, `avgbicycle`, `slowbicycle`) as `bicycle`\n",
    "- Rename `trailer` and `truck` as `lorry` as trailer is just a variation of truck\n",
    "- Rename `uber` and `taxi` as `hire` as they represent the same concept\n",
    "- Rename `army`, `authority`, `emergency` vehicles as `special`\n",
    "- Drop all vehicle types starting with `hw_` as these vehicles run on motorways and hence not suitable for urban traffic analysis\n",
    "- Drop rows containing `train` as they are not suitable for urban traffic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hw_trailer', 'hw_delivery', 'hw_passenger3', 'hw_truck',\n",
       "       'hw_passenger2b', 'hw_coach', 'hw_motorcycle', 'hw_passenger1',\n",
       "       'hw_passenger4', 'hw_passenger2a', 'bus', 'delivery', 'coach',\n",
       "       'truck', 'trailer', 'avgbicycle', 'slowbicycle', 'fastbicycle',\n",
       "       'motorcycle', 'taxi', 'moped', 'uber', 'passenger3', 'passenger4',\n",
       "       'passenger2a', 'passenger1', 'passenger2b', 'emergency',\n",
       "       'authority', 'army', 'train'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[TARGET_COL].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20675 rows with vehicle_type starting with 'hw_' or are 'train' dropped. 1208287 rows remain.\n"
     ]
    }
   ],
   "source": [
    "# Rename vehicle types\n",
    "df[TARGET_COL] = df[TARGET_COL].replace(regex={\n",
    "    '^passenger.*': 'car',\n",
    "    '.*bicycle$': 'bicycle',\n",
    "    '^(trailer|truck)$': 'lorry',\n",
    "    '^(uber|taxi)$': 'hire',\n",
    "    '^(army|authority|emergency)$': 'special'\n",
    "})\n",
    "\n",
    "# Drop rows where vehicle types start with 'hw_' or are 'train'\n",
    "bef = len(df)\n",
    "df = df[~df[TARGET_COL].str.startswith('hw_')]\n",
    "df = df[df[TARGET_COL] != 'train']\n",
    "aft = len(df)\n",
    "print(f\"{bef - aft} rows with vehicle_type starting with 'hw_' or are 'train' dropped. {aft} rows remain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bus', 'delivery', 'coach', 'lorry', 'bicycle', 'motorcycle',\n",
       "       'hire', 'moped', 'car', 'special'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "df[TARGET_COL].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1208287 entries, 0 to 1208286\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count    Dtype         \n",
      "---  ------                --------------    -----         \n",
      " 0   unique_id             1208287 non-null  object        \n",
      " 1   datetime              1208287 non-null  datetime64[ns]\n",
      " 2   vehicle_id            1208287 non-null  object        \n",
      " 3   timestep_time         1208287 non-null  float64       \n",
      " 4   vehicle_type          1208287 non-null  object        \n",
      " 5   vehicle_speed         1208287 non-null  float64       \n",
      " 6   vehicle_x             1208287 non-null  float64       \n",
      " 7   vehicle_y             1208287 non-null  float64       \n",
      " 8   vehicle_z             1208287 non-null  float64       \n",
      " 9   vehicle_angle         1208287 non-null  float64       \n",
      " 10  vehicle_angle_sine    1208287 non-null  float64       \n",
      " 11  vehicle_angle_cosine  1208287 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(8), object(3)\n",
      "memory usage: 110.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Sort by 'unique_id' and 'datetime' to ensure the correct order\n",
    "df = df.sort_values(by=[GROUPBY_COL, DATETIME_COL]).reset_index(drop=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df[TARGET_COL] = df[TARGET_COL].apply(str)\n",
    "df[TARGET_COL] = le.fit_transform(df[TARGET_COL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'bicycle',\n",
      " 1: 'bus',\n",
      " 2: 'car',\n",
      " 3: 'coach',\n",
      " 4: 'delivery',\n",
      " 5: 'hire',\n",
      " 6: 'lorry',\n",
      " 7: 'moped',\n",
      " 8: 'motorcycle',\n",
      " 9: 'special'}\n"
     ]
    }
   ],
   "source": [
    "label_mapping = dict(zip(range(len(le.classes_)), le.classes_))\n",
    "pprint.pprint(label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train val test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = train_val_test_split_custom(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect balance of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5886 vehicle_ids and 1208287 rows in df.\n",
      "10 classes are represented in df.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {df[GROUPBY_COL].nunique()} vehicle_ids and {len(df)} rows in df.\")\n",
    "print(f\"{df[TARGET_COL].nunique()} classes are represented in df.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_ids\n",
      "vehicle_type\n",
      "2    2922\n",
      "8    1000\n",
      "0     465\n",
      "7     451\n",
      "4     317\n",
      "5     240\n",
      "1     192\n",
      "9     191\n",
      "6      79\n",
      "3      29\n",
      "Name: unique_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Inspect balance of classes by number of unique_ids\n",
    "print('unique_ids')\n",
    "print(f\"{df.groupby(TARGET_COL)[GROUPBY_COL].nunique().sort_values(ascending=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows\n",
      "vehicle_type\n",
      "2    744232\n",
      "8    210160\n",
      "7     91608\n",
      "1     39480\n",
      "0     37562\n",
      "4     36238\n",
      "9     20933\n",
      "5     14208\n",
      "6     11160\n",
      "3      2706\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Inspect balance of classes by number of rows\n",
    "print('num_rows')\n",
    "print(f\"{df[TARGET_COL].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4110 vehicle_ids and 829958 rows in train_df.\n",
      "10 classes are represented in train_df.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {train_df[GROUPBY_COL].nunique()} vehicle_ids and {len(train_df)} rows in train_df.\")\n",
    "print(f\"{train_df[TARGET_COL].nunique()} classes are represented in train_df.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_ids\n",
      "vehicle_type\n",
      "2    2042\n",
      "8     674\n",
      "0     330\n",
      "7     319\n",
      "4     237\n",
      "5     172\n",
      "9     133\n",
      "1     130\n",
      "6      56\n",
      "3      17\n",
      "Name: unique_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Inspect balance of classes by number of unique_ids\n",
    "print('unique_ids')\n",
    "print(f\"{train_df.groupby(TARGET_COL)[GROUPBY_COL].nunique().sort_values(ascending=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows\n",
      "vehicle_type\n",
      "2    516604\n",
      "8    137658\n",
      "7     63953\n",
      "0     26798\n",
      "4     26711\n",
      "1     25681\n",
      "9     13695\n",
      "5      9857\n",
      "6      7502\n",
      "3      1499\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Inspect balance of classes by number of rows\n",
    "print('num_rows')\n",
    "print(f\"{train_df[TARGET_COL].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 595 vehicle_ids and 126543 rows in val_df.\n",
      "10 classes are represented in val_df.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {val_df[GROUPBY_COL].nunique()} vehicle_ids and {len(val_df)} rows in val_df.\")\n",
    "print(f\"{val_df[TARGET_COL].nunique()} classes are represented in val_df.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_ids\n",
      "vehicle_type\n",
      "2    277\n",
      "8    111\n",
      "7     53\n",
      "0     46\n",
      "4     28\n",
      "1     25\n",
      "5     21\n",
      "9     20\n",
      "6      8\n",
      "3      6\n",
      "Name: unique_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Inspect balance of classes by number of unique_ids\n",
    "print('unique_ids')\n",
    "print(f\"{val_df.groupby(TARGET_COL)[GROUPBY_COL].nunique().sort_values(ascending=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows\n",
      "vehicle_type\n",
      "2    72938\n",
      "8    24916\n",
      "7    10983\n",
      "1     5054\n",
      "0     3747\n",
      "4     3128\n",
      "9     2157\n",
      "6     1686\n",
      "5     1524\n",
      "3      410\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Inspect balance of classes by number of rows\n",
    "print('num_rows')\n",
    "print(f\"{val_df[TARGET_COL].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1181 vehicle_ids and 251786 rows in test_df.\n",
      "10 classes are represented in test_df.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {test_df[GROUPBY_COL].nunique()} vehicle_ids and {len(test_df)} rows in test_df.\")\n",
    "print(f\"{test_df[TARGET_COL].nunique()} classes are represented in test_df.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_ids\n",
      "vehicle_type\n",
      "2    603\n",
      "8    215\n",
      "0     89\n",
      "7     79\n",
      "4     52\n",
      "5     47\n",
      "9     38\n",
      "1     37\n",
      "6     15\n",
      "3      6\n",
      "Name: unique_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Inspect balance of classes by number of unique_ids\n",
    "print('unique_ids')\n",
    "print(f\"{test_df.groupby(TARGET_COL)[GROUPBY_COL].nunique().sort_values(ascending=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows\n",
      "vehicle_type\n",
      "2    154690\n",
      "8     47586\n",
      "7     16672\n",
      "1      8745\n",
      "0      7017\n",
      "4      6399\n",
      "9      5081\n",
      "5      2827\n",
      "6      1972\n",
      "3       797\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Inspect balance of classes by number of rows\n",
    "print('num_rows')\n",
    "print(f\"{test_df[TARGET_COL].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Balance classes if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample majority class and upsample minority class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform the train, val and test data\n",
    "scaler.fit(train_df[FEATURE_COLS])\n",
    "train_df.loc[:, FEATURE_COLS] = scaler.transform(train_df[FEATURE_COLS])\n",
    "val_df.loc[:, FEATURE_COLS] = scaler.transform(val_df[FEATURE_COLS])\n",
    "test_df.loc[:, FEATURE_COLS] = scaler.transform(test_df[FEATURE_COLS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by 'unique_id' and 'datetime' to ensure the correct order\n",
    "train_df = train_df.sort_values(by=[GROUPBY_COL, DATETIME_COL]).reset_index(drop=True)\n",
    "val_df = val_df.sort_values(by=[GROUPBY_COL, DATETIME_COL]).reset_index(drop=True)\n",
    "test_df = test_df.sort_values(by=[GROUPBY_COL, DATETIME_COL]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save train, val, and test dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train, val, test data saved as .csv file.\n"
     ]
    }
   ],
   "source": [
    "train_df.to_csv(f'train_data.csv', index=False)\n",
    "val_df.to_csv(f'val_data.csv', index=False)\n",
    "test_df.to_csv(f'test_data.csv', index=False)\n",
    "print(\"Train, val, test data saved as .csv file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reshaping data for input to LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating datasets and dataloaders\n",
    "train_dataset = VehicleDataset(train_df)\n",
    "val_dataset = VehicleDataset(val_df)\n",
    "test_dataset = VehicleDataset(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "# test_loader1 = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# assert(len(test_loader) == len(test_loader1))\n",
    "# for i, batch in enumerate(test_loader):\n",
    "#     for j, batch1 in enumerate(test_loader1):\n",
    "#         if (i == j):\n",
    "#             assert(torch.all(torch.eq(batch[0], batch1[0])))\n",
    "#             assert(torch.all(torch.eq(batch[1], batch1[1])))\n",
    "#             assert(batch[2] == batch1[2])\n",
    "#             assert(torch.all(torch.eq(batch[3], batch1[3])))\n",
    "#         else:\n",
    "#             continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "duolux_v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
